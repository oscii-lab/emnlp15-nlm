\section{Experimental Results}
\label{sec:exp}

We train on the English-French parallel corpora from ACL WMT '14, which contains
348M words. To evaluate performance, we use the test set news-test-2014, also from
WMT '14, which contains 3003 new sentences. As a comparison metric, we additionally run the system presented by
\newcite{journals/corr/BahdanauCB14} using the same parameters and data. We ensure
that after our data transformation is applied, the resulting vocabulary also has size
30,000, matching the number used in their experiments. We construct the trees by the following method. First, we use
the formulas derived in the previous section to calculate the optimal number of
common words $W$ for each encoding scheme, using $V$ to be the vocabulary size of the training corpus and $K = 30000$.
We then find the $W$ words in the text with highest frequency and
apply the identity transformation to them. For the remaining words, we encode them
using a distinct symbol whose form matches the one prescribed for each
encoding scheme.

We use the RNNsearch-50 architecture from \newcite{journals/corr/BahdanauCB14} as
our black box machine translation system. We report results for this system alone,
as well as for each of our three encoding schemes, using the BLEU score metric (\newcite{bleu}) for
comparison with previously reported scores. Our results after training
for about 5 days are compiled in Tables \ref{table:results} and \ref{table:precs}.

Our results indicate that the scheme which keeps the highest number of common words, \emph{Repeat-All},
performs best. The precision values (proportion of words in the test set translation that appear in the
reference corpus) reveal that the common word translation accuracy does not change significantly
among strategies, but the system learns translations for the rare words best when they are encoded
using \emph{Repeat-All}.

\begin{table}
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{cccc}
    \toprule
      \textbf{Encoding} & \textbf{BLEU} & \textbf{\# Common Words}\\
    \midrule
      None & 0.2577 & 30000\\
    \midrule
      Repeat-All & 0.2745 & 29940\\
    \midrule
      Repeat-Symbol & 0.2652 & 28860\\
    \midrule
      No-Repeats & 0.2579 & 27320\\
    \bottomrule
  \end{tabular}}
  \caption{BLEU scores on detokenized test set for various encoding
    schemes after training for 5 days.}
  \label{table:results}
\end{table}

\begin{table}
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{cccc}
    \toprule
      \textbf{Encoding} & \textbf{CW Precision} & \textbf{RW Precision}\\
    \midrule
      None & 0.895 & N/A\\
    \midrule
      Repeat-All & 0.970 & 0.332\\
    \midrule
      Repeat-Symbol & 0.967 & 0.216\\
    \midrule
      No-Repeats & 0.970 & 0.210\\
    \bottomrule
  \end{tabular}}
  \caption{Test set precision on common words (CW) and rare words (RW) per encoding strategy.}
  \label{table:precs}
\end{table}
