\section{Experimental Results}
\label{sec:exp}

We trained a public
implementation\footnote{\url{github.com/lisa-groundhog/GroundHog}} of the
system described in \newcite{journals/corr/BahdanauCB14} on the English-French
parallel corpus from ACL WMT 2014, which contains 348M tokens. We evaluated on
news-test-2014, also from WMT 2014, which contains 3003 sentences. All
experiments used the same learning parameters and vocabulary size of 30,000.

We constructed each encoding by the following method. First, we used the
formulas derived in the previous section to calculate the optimal number of
common words $W$ for each encoding scheme, using $V$ to be the true vocabulary
size of the training corpus and $W = 30,000$. We then found the $C$ most common
words in the text and encoded them as themselves. For the remaining rare words,
we encoded them using a distinct symbol whose form matched the one prescribed
for each encoding scheme. The encoding was then applied separately to both the
source text and the target text. Our encoding schemes all increased the total
number of tokens in the training corpus by approximately 4\%.

To construct the mapping from rare words to their 2-word encodings, we binned rare
words by frequency into branches. Thus, rare words of similar frequency in the
training corpus tended to have encodings with the same first symbol. Similarly,
the standard Huffman construction algorithm groups together rare words with
similar frequencies within subtrees. More intelligent heuristics for
constructing trees, such as using translation statistics instead of training
corpus frequency, would be an interesting area of future work.

\subsection{Results}

\begin{table}
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{|l|l|l|l|}
    \hline
      \textbf{Encoding} & \textbf{BLEU} & \textbf{\# Common Words}\\
    \hline
      None & 25.77 & 30,000\\
    \hline
      Repeat-All & 27.45 & 29,940\\
    \hline
      Repeat-Symbol & 26.52 & 28,860\\
    \hline
      No-Repeats & 25.79 & 27,320\\
    \hline
  \end{tabular}}
  \caption{BLEU scores (\%) on detokenized test set for each encoding
    scheme after training for 5 days.}
  \label{table:results}
\end{table}

We used the RNNsearch-50 architecture from \newcite{journals/corr/BahdanauCB14}
as our machine translation system. We report results for this system alone, as
well as for each of our three encoding schemes, using the BLEU metric
\cite{bleu}. Table~\ref{table:results} summarizes our results after training
each variant for 5 days, corresponding to roughly 2 passes through the
180K-sentence training corpus.

Alternative techniques that leverage bilingual resources have been shown to
provide larger improvements. \newcite{journals/corr/JeanCMB14} demonstrate an
improvement of 3.1 BLEU by using bilingual word cooccurrence statistics in an
aligned corpus to replace \emph{unknown word} tokens.
\newcite{journals/corr/LuongSLVZ14} demonstrate an improvement of up to 2.8
BLEU over a series of stronger baselines using an unknown word model that
also makes predictions using a bilingual dictionary.

\subsection{Analysis}

\begin{table}
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{|l|l|l|l|}
    \hline
      \textbf{Encoding} & \textbf{Common} & \textbf{Rare} & \textbf{1st Symbol}\\
    \hline
      None & 62.0 & 0.0 & -\\
    \hline
      Repeat-All & 65.8 & 28.0 & 64.8 \\
    \hline
      Repeat-Symbol & 65.5 & 16.5 & 24.8 \\
    \hline
      No-Repeats & 63.6 & 15.8 & 25.7 \\
    \hline
  \end{tabular}}
  \caption{Test set precision (\%) on common words and rare words for each
  encoding strategy. \emph{1st Symbol} denotes the precision of the first
  pseudo-word symbol in an encoded rare word.}
  \label{table:precs}
\end{table}

Our results indicate that the encoding scheme that keeps the highest number of
common words, \emph{Repeat-All}, performs best. Table~\ref{table:precs} shows
the unigram precision of each output. The common word translation accuracy is
higher for all encoding schemes than for the baseline, although all precisions
are similar. Larger differences appear in the precision of rare words. The
scheme that encodes rare words using both pseudo-words and common words gives
substantially higher rare word accuracy than any other approach.

The final column of Table~\ref{table:precs} shows the unigram precision of the
first pseudo-word in an encoded rare word. The \emph{Repeat-All} scheme uses
only 60 different first symbols to encode all rare words. The other schemes
require over 1,000. The fact that \emph{Repeat-All} has a constrained set of
rare word first symbols may account for its higher rare word precision.

It is possible for the model to predict an invalid encoded sequence that does
not correspond to anything in the original vocabulary. However, in our
experiments, we did not observe any invalid sequences in the decoding of the
test set. A reasonable way to deal with invalid sequences would be to drop them
from the output during decoding.

