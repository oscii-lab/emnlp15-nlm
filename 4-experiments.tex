\section{Experimental Results}
\label{sec:exp}

We train the public implementation of the system descrbed in
\newcite{journals/corr/BahdanauCB14} on the English-French parallel corpora
from ACL WMT '14, which contains 348M tokens. We evaluate on news-test-2014,
also from WMT '14, which contains 3003 sentences. All experiments use the same
learning parameters and vocabulary size of 30,000.

We construct each encoding by the following method. First, we use the formulas
derived in the previous section to calculate the optimal number of common words
$W$ for each encoding scheme, using $V$ to be the true vocabulary size of the
training corpus and $W = 30,000$. We then find the $C$ most common words in the
text and encode them as themselves. For the remaining rare words, we encode
them using a distinct symbol whose form matches the one prescribed for each
encoding scheme. Rare words are binned by frequency into branches.

We use the RNNsearch-50 architecture from \newcite{journals/corr/BahdanauCB14}
as our machine translation system. We report results for this system alone, as
well as for each of our three encoding schemes, using the BLEU metric
\cite{bleu}. Our results after training each variant for 5 days appear in
Table~\ref{table:results}.

Our results indicate that the scheme that keeps the highest number of common
words, \emph{Repeat-All}, performs best. Table~\ref{table:precs} shows the
unigram precision of each output. The common word translation accuracy does not
change substantially among strategies, but encoding rare words using both
pseudo-words and common words gives substantially higher rare word accuracy.

\begin{table}
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{cccc}
    \toprule
      \textbf{Encoding} & \textbf{BLEU} & \textbf{\# Common Words}\\
    \midrule
      None & 0.2577 & 30000\\
    \midrule
      Repeat-All & 0.2745 & 29940\\
    \midrule
      Repeat-Symbol & 0.2652 & 28860\\
    \midrule
      No-Repeats & 0.2579 & 27320\\
    \bottomrule
  \end{tabular}}
  \caption{BLEU scores on detokenized test set for various encoding
    schemes after training for 5 days.}
  \label{table:results}
\end{table}

\begin{table}
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{cccc}
    \toprule
      \textbf{Encoding} & \textbf{CW Precision} & \textbf{RW Precision}\\
    \midrule
      None & 0.620 & N/A\\
    \midrule
      Repeat-All & 0.658 & 0.280\\
    \midrule
      Repeat-Symbol & 0.655 & 0.165\\
    \midrule
      No-Repeats & 0.636 & 0.132\\
    \bottomrule
  \end{tabular}}
  \caption{Test set precision on common words (CW) and rare words (RW) per encoding strategy.}
  \label{table:precs}
\end{table}
