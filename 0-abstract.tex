Recent work in neural machine translation has shown promising performance, but some effective
architectures do not scale naturally to large vocabulary sizes because each
word in the vocabulary must be embedded into a vector space. We propose and compare
three variable-length encoding schemes for a large-vocabulary machine
translation task. Common words are unaffected by our encoding, but rare
words are encoded using a sequence of two pseudo-words. In comparison to
replacing rare words with a special \emph{unknown word} symbol, our best
variable-length encoding strategy improves English-French translation
performance by X.X\% BLEU. TODO
