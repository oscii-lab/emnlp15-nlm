\section{Introduction}
\label{sec:intro}

\newcite{journals/corr/BahdanauCB14} proposes a neural translation model
that learns vector representations for individual words as well as word
sequences. Their approach jointly predicts a translation and a latent
word-level alignment for a sequence of source words. However, the architecture
of the network does not scale naturally to large vocabularies
\cite{journals/corr/JeanCMB14}.

In this paper, we propose a novel approach to circumvent the large-vocabulary
challenge by preprocessing the source and target word sequences, encoding them
as a longer token sequence drawn from a small vocabulary that does not
discard any information. Common words are unaffected, but rare words are
encoded as a sequence of two pseudo-words. The exact same learning and
inference machinery applied to these transformed data yields improved
translations.

Our approach considers a family of 3 different encoding schemes based on
Huffman codes. All of them  eliminate the need to replace every rare word with
the \emph{unknown word} symbol. Our approach is simpler than other methods
recently proposed to address the same issue. It does not introduce new
parameters into the model, change the model structure, affect inference,
require access to a complete dictionary, or require any additional learning
procedures. Nonetheless, compared to a baseline system that replaces all rare
words with an \emph{unknown word} symbol, our methods improves large-scale
English-French news translation by up to 1.7 BLEU.
