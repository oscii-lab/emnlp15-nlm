Thank you for your comments and feedback! We've answered some of your open questions below, and we will incorporate these responses and all of your suggestions into the final version of the paper.

Invalid sequences: As the reviews pointed out, it is possible for the model to predict an invalid encoded sequence that does not correspond to anything in the original vocabulary. However, we did not observe any invalid sequences in the decoding of the test set. A reasonable way to deal with invalid sequences would be to drop them from the output during decoding.

Convergence: 5 days of training allowed us to perform about 2 passes through the 180K sentence training corpus in each experiment. In subsequent days, performance did not improve for Repeat-All, but improved slightly for Repeat-Symbol and No-Repeats. We will include a discussion of convergence in the final version of the paper.

Analysis: The performance of different encoding schemes may stem from the different levels of ambiguity of the encoded tokens. For example, in the Repeat-All scheme, each terminal symbol (e.g., "not") is part of the encoding of one common word and 60 rare words. In the No-Repeats scheme, a terminal symbol for rare words (e.g., "t1") is part of 2,680 different rare words. 

The statement "rare words are binned by frequency into branches" refers to when we are building the mapping from rare words to their 2-word encodings for Repeat-Symbol and No-Repeats. For these schemes, rare words of similar frequency in the training corpus are encoded to symbols with the same first symbol. Similarly, the standard Huffman construction algorithm groups together rare words with similar frequencies within subtrees. More intelligent heuristics for constructing trees (e.g., using translation statistics instead of training corpus frequency) will be an interesting area of future work.
