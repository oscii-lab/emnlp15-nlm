Neural machine translation has shown promising performance, but some effective
architectures do not scale naturally to large vocabulary sizes because each
vocabulary item must be embedded into a vector space. We propose and compare
three variable-length encoding schemes for a large-vocabulary machine
translation task. Common words are represented in the standard manner, but rare
words are encoded using a sequence of two pseudo words. In comparison to
replacing rare words with an \emph{unknown word} symbol, our best
variable-length encoding strategy improves English-French translation
performance by X.X\% BLEU.
