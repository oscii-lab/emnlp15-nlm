\section{Background}
\label{sec:background}

\subsection{Neural Machine Translation}
The conventional architecture for alignment-based neural machine translation, as proposed by \newcite{journals/corr/ChoMGBSB14},
consists of an encoder and a decoder, both recurrent neural networks. The encoder receives
a source text sentence $\mathbf{x}$ and outputs a fixed-length vector of hidden states:
\begin{equation}
h_{t} = f(x_{t}, h_{t-1})
\end{equation},
where $f$ is any non-linear activation function. The decoder then converts these hidden state vectors to
target symbols $\mathbf{y}$ according to
\begin{equation}
P(y_{t} | y_{t-1}, ..., y_{1}, \mathbf{x}) = g(h_{t}, y_{t-1}, c)
\end{equation},
where $c$ is a \emph{summary} of the entire input sequence and $g$ is another activation function.
Naturally, the encoder and decoder parameters are jointly optimized to maximize the log likelihood:
\begin{equation}
\theta^{*} = \argmax_{\theta} \sum_{n=1}^{N}\log p(y_{n} | x_{n}, \theta)
\end{equation}

In \newcite{journals/corr/BahdanauCB14}, the authors propose a system to relieve the encoder of its need
to convert each source sentence into a fixed-length hidden state vector. Toward this end, their approach builds an attention mechanism
into the decoder, which is trained to determine on which portions of the source sentence to focus. The encoder
is built as a bidirectional recurrent neural network, where the hidden state annotations $h_{t}$ for each source text word
are the result of concatenating forward hidden states $\overrightarrow{h_{t}} = f(x_{t}, \overrightarrow{h_{t-1}})$ and backward hidden states
$\overleftarrow{h_{t}} = f(x_{t}, \overleftarrow{h_{t+1}})$. The decoder computes $c_{t}$, the summarizing context vector,
as a convex combination of the $h_{t}$, with coefficients equal to $a(h_{t}, z_{t-1})$ scaled between 0 and 1 using a softmax computation, where $a$ is a
jointly trained feedforward neural network with one hidden layer. We may thus write
\begin{equation}
P(y_{t} | y_{t-1}, ..., y_{1}, \mathbf{x}) = \dfrac{exp\{\theta_{t}^{T} \phi(y_{t-1}, z_{t}, c_{t})\}}{\displaystyle\sum_{s:y_{s} \in V} exp\{\theta_{s}^{T} \phi(y_{t-1}, z_{t}, c_{t})\}}
\end{equation}
for the probability of the next target vocabulary word in the translation.\\

As detailed by \newcite{journals/corr/JeanCMB14}, the dependence of the complexity on vocabulary
size stems from the computation of the normalization factor in equation (4). This leads
to significant temporal and memory requirements that scale with increased vocabulary size: storage space and
learning time are both increased. Our approach thus uses this neural machine
translation system as a black box while transforming the text to have a fixed vocabulary size.

\subsection{Related Work}
Typically, neural machine translation systems only consider the top 30K to 100K words in the text, replacing the other words
with a filler symbol. There has been much recent work in improving translation quality with a large vocabulary size.
\newcite{journals/corr/LuongSLVZ14} describe an approach
that, similar to ours, treats the translation system as a black box. They eliminate unknown symbols by training the
system to recognize from where in the source text each unknown word in the target text came, so that in a postprocessing
 phase, the unknown word can be replaced by a dictionary lookup of the corresponding source text word. In contrast,
our method does not rely on having a complete dictionary, and instead transforms the data to allow the system itself to
learn translations for even the rare words.\\

Some approaches have altered the model itself to circumvent the expensive normalization computation, rather than
applying preprocessing and postprocessing on the text. \newcite{journals/corr/JeanCMB14}
develop an importance sampling strategy for estimating the exponent term in equation (4), thus leading to
an approximation of the softmax computation. \newcite{NIPS2013_5165}
present a technique for approximation of target word probability using noise-contrastive estimation. Our method can
be applied in conjunction with these techniques.\\

Sequential or hierarchical encodings of large vocabularies have played an
improtant role in recurrent neural network language models, primarily to
address the inference time issue of large vocabularies. \newcite{5947611}
describe an architecture in which output word types are grouped into classes by
frequency, and the network first predicts a class and then a word in that
class. \newcite{journals/corr/abs-1301-3781} describe an encoding of the output
vocabulary as a binary tree. To our knowledge, hierarchical encodings have not
been applied to input vocabulary encoding for translation.
% TODO Cite some more language model papers [sent Rohan an email]

\subsection{Huffman Codes}

An encoding can be used to represent a sequence of tokens from a large
vocabulary $\mathcal{V}$ using a small vocabulary $\mathcal{W}$.  In the case
of translation, let $\mathcal{V}$ be the original corpus vocabulary, which can
number in the millions of word types in a typical corpus. Let $\mathcal{W}$ be
the limited vocabulary size of a neural translation model, which is typically
set to a much smaller number such as 30,000. Let $V=|\mathcal{V}|$ and
$W=|\mathcal{W}|$.

A deterministically invertible, variable-length encoding maps each
$v\in\mathcal{V}$ to a sequence $w \in \mathcal{W}+$ such that no other
$v'\in\mathcal{V}$ is mapped to a prefix of $w$. Encoding simply replaces each
element of $V$ with its encoding, and decoding is unambiguous because of this
prefix restriction. An encoding can be represented as a tree in which each leaf
corresponds to an element of $V$, each node contains a symbol from
$\mathcal{W}$, and the encoding of any leaf is its path from the root.

A Huffman code is an optimal encoding that uses as few symbols from
$\mathcal{W}$ as possible to encode an original sequence of symbols from
$\mathcal{V}$. Although binary codes are typical, $\mathcal{W}$ can have any
size. An optimal encoding can be found using a greedy algorithm
\cite{huffman}.
