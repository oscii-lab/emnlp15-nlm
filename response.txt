Thank you for your comments and feedback! We've answered some of your open questions below, and we will incorporate these responses and all your suggestions into the final version of the paper.

Regarding whether the translation on the test set contained "broken" encoded sequences: this is a good point that we did not address. We did not encounter any such unrecoverable sequences in the decoding of the test set translation, likely because such a sequence did not show up during training. A reasonable way to deal with this if it did occur would be to drop broken encodings while doing the decoding.

In 5 days, we completed about 300,000 training iterations, and performance had converged for Repeat-All. However, Repeat-Symbol and No-Repeats showed continued improvement over subsequent days.

The relative benefits of the three variants stem from the different levels of ambiguity in the tokens. For example, in the No-Repeats scheme, any symbol of the form "sX" must be the first part of a rare word encoding, and "tX" must be the second part. Thus, anytime there is a "sX" token, it will always be followed by "tX." It is conceivable that the network learns to take advantage of such patterns introduced by the encoding scheme.

The sentence "Rare words are binned by frequency into branches" refers to when we are building the mapping from rare words to their 2-word encodings for Repeat-Symbol and No-Repeats. For these schemes, our loops are set up so that rare words of similar frequency in the training corpus are encoded to symbols with the same first word. On the other hand, Repeat-All is a standard Huffman code, so for this we use a priority queue based on frequency to assign rare words to their encodings. Determining intelligent heuristics for constructing this mapping (using the actual word translations instead of training corpus frequency) will be our next area of focus.
